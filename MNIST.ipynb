{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Importing useful libraries\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"MNIST.csv\") #Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(data.iloc[:,0]).values #One-hot encoding for labels\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "X = data.iloc[:,1:]\n",
    "X = X.values.reshape(42000,28,28,1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualizing an image from the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "plt.imshow(X[5,:,:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset into training and testing dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2,random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, BatchNormalization, Dropout, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using ImageDataGenerator library for data augmentation\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(rescale = 1./255, zoom_range = 0.1, shear_range = 0.02, rotation_range = 10, width_shift_range=0.1, height_shift_range=0.1)\n",
    "datagen.fit(X_train)\n",
    "valgen = ImageDataGenerator(rescale = 1./255)\n",
    "valgen.fit(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing LeNet-5 architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(6,(5,5), padding = 'valid', activation = 'relu',input_shape=(28,28,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(16,(5,5), padding = 'same', activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120,activation = 'relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(84,activation = 'relu'))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(10,activation = tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using ModelCheckpoint so that we do not have to worry about overfitting due to training for too many epochs\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath=\"mnist.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',metrics = [\"accuracy\"],loss = \"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "65/65 [==============================] - 9s 138ms/step - loss: 1.0372 - acc: 0.6685 - val_loss: 0.3063 - val_acc: 0.8988\n",
      "Epoch 2/200\n",
      "65/65 [==============================] - 8s 119ms/step - loss: 0.3448 - acc: 0.9022 - val_loss: 0.1800 - val_acc: 0.9435\n",
      "Epoch 3/200\n",
      "65/65 [==============================] - 7s 115ms/step - loss: 0.2178 - acc: 0.9374 - val_loss: 0.1246 - val_acc: 0.9617\n",
      "Epoch 4/200\n",
      "65/65 [==============================] - 7s 114ms/step - loss: 0.1714 - acc: 0.9494 - val_loss: 0.0936 - val_acc: 0.9719\n",
      "Epoch 5/200\n",
      "65/65 [==============================] - 8s 118ms/step - loss: 0.1390 - acc: 0.9600 - val_loss: 0.0699 - val_acc: 0.9782\n",
      "Epoch 6/200\n",
      "65/65 [==============================] - 8s 121ms/step - loss: 0.1265 - acc: 0.9623 - val_loss: 0.0658 - val_acc: 0.9787\n",
      "Epoch 7/200\n",
      "65/65 [==============================] - 10s 149ms/step - loss: 0.1149 - acc: 0.9651 - val_loss: 0.0614 - val_acc: 0.9815\n",
      "Epoch 8/200\n",
      "65/65 [==============================] - 16s 241ms/step - loss: 0.1026 - acc: 0.9693 - val_loss: 0.0659 - val_acc: 0.9806\n",
      "Epoch 9/200\n",
      "65/65 [==============================] - 15s 237ms/step - loss: 0.0949 - acc: 0.9715 - val_loss: 0.0636 - val_acc: 0.9806\n",
      "Epoch 10/200\n",
      "65/65 [==============================] - 16s 239ms/step - loss: 0.0882 - acc: 0.9731 - val_loss: 0.0628 - val_acc: 0.9803\n",
      "Epoch 11/200\n",
      "65/65 [==============================] - 16s 239ms/step - loss: 0.0841 - acc: 0.9750 - val_loss: 0.0503 - val_acc: 0.9848\n",
      "Epoch 12/200\n",
      "65/65 [==============================] - 16s 241ms/step - loss: 0.0824 - acc: 0.9745 - val_loss: 0.0514 - val_acc: 0.9852\n",
      "Epoch 13/200\n",
      "65/65 [==============================] - 16s 239ms/step - loss: 0.0791 - acc: 0.9755 - val_loss: 0.0477 - val_acc: 0.9867\n",
      "Epoch 14/200\n",
      "65/65 [==============================] - 16s 239ms/step - loss: 0.0758 - acc: 0.9781 - val_loss: 0.0634 - val_acc: 0.9800\n",
      "Epoch 15/200\n",
      "65/65 [==============================] - 16s 239ms/step - loss: 0.0727 - acc: 0.9777 - val_loss: 0.0444 - val_acc: 0.9885\n",
      "Epoch 16/200\n",
      "65/65 [==============================] - 16s 238ms/step - loss: 0.0658 - acc: 0.9801 - val_loss: 0.0502 - val_acc: 0.9859\n",
      "Epoch 17/200\n",
      "65/65 [==============================] - 16s 240ms/step - loss: 0.0675 - acc: 0.9804 - val_loss: 0.0418 - val_acc: 0.9882\n",
      "Epoch 18/200\n",
      "65/65 [==============================] - 15s 238ms/step - loss: 0.0621 - acc: 0.9809 - val_loss: 0.0571 - val_acc: 0.9835\n",
      "Epoch 19/200\n",
      "65/65 [==============================] - 15s 237ms/step - loss: 0.0619 - acc: 0.9803 - val_loss: 0.0462 - val_acc: 0.9877\n",
      "Epoch 20/200\n",
      "65/65 [==============================] - 16s 241ms/step - loss: 0.0621 - acc: 0.9810 - val_loss: 0.0478 - val_acc: 0.9867\n",
      "Epoch 21/200\n",
      "65/65 [==============================] - 16s 239ms/step - loss: 0.0610 - acc: 0.9815 - val_loss: 0.0854 - val_acc: 0.9736\n",
      "Epoch 22/200\n",
      "65/65 [==============================] - 15s 237ms/step - loss: 0.0579 - acc: 0.9816 - val_loss: 0.0391 - val_acc: 0.9883\n",
      "Epoch 23/200\n",
      "65/65 [==============================] - 15s 236ms/step - loss: 0.0583 - acc: 0.9822 - val_loss: 0.0413 - val_acc: 0.9878\n",
      "Epoch 24/200\n",
      "65/65 [==============================] - 15s 238ms/step - loss: 0.0529 - acc: 0.9836 - val_loss: 0.0452 - val_acc: 0.9859\n",
      "Epoch 25/200\n",
      "65/65 [==============================] - 15s 234ms/step - loss: 0.0538 - acc: 0.9839 - val_loss: 0.0646 - val_acc: 0.9803\n",
      "Epoch 26/200\n",
      "65/65 [==============================] - 16s 240ms/step - loss: 0.0520 - acc: 0.9842 - val_loss: 0.0477 - val_acc: 0.9857\n",
      "Epoch 27/200\n",
      "65/65 [==============================] - 16s 239ms/step - loss: 0.0522 - acc: 0.9844 - val_loss: 0.0332 - val_acc: 0.9904\n",
      "Epoch 28/200\n",
      "65/65 [==============================] - 15s 237ms/step - loss: 0.0504 - acc: 0.9848 - val_loss: 0.0437 - val_acc: 0.9864\n",
      "Epoch 29/200\n",
      "65/65 [==============================] - 15s 236ms/step - loss: 0.0457 - acc: 0.9851 - val_loss: 0.0501 - val_acc: 0.9855\n",
      "Epoch 30/200\n",
      "65/65 [==============================] - 15s 237ms/step - loss: 0.0476 - acc: 0.9858 - val_loss: 0.0466 - val_acc: 0.9849\n",
      "Epoch 31/200\n",
      "65/65 [==============================] - 12s 188ms/step - loss: 0.0473 - acc: 0.9857 - val_loss: 0.0368 - val_acc: 0.9907\n",
      "Epoch 32/200\n",
      "65/65 [==============================] - 7s 101ms/step - loss: 0.0466 - acc: 0.9860 - val_loss: 0.0388 - val_acc: 0.9901\n",
      "Epoch 33/200\n",
      "65/65 [==============================] - 7s 103ms/step - loss: 0.0435 - acc: 0.9872 - val_loss: 0.0352 - val_acc: 0.9886\n",
      "Epoch 34/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0468 - acc: 0.9851 - val_loss: 0.0416 - val_acc: 0.9876\n",
      "Epoch 35/200\n",
      "65/65 [==============================] - 9s 131ms/step - loss: 0.0452 - acc: 0.9869 - val_loss: 0.0380 - val_acc: 0.9897\n",
      "Epoch 36/200\n",
      "65/65 [==============================] - 9s 131ms/step - loss: 0.0404 - acc: 0.9877 - val_loss: 0.0395 - val_acc: 0.9883\n",
      "Epoch 37/200\n",
      "65/65 [==============================] - 9s 131ms/step - loss: 0.0411 - acc: 0.9873 - val_loss: 0.0407 - val_acc: 0.9882\n",
      "Epoch 38/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0419 - acc: 0.9869 - val_loss: 0.0390 - val_acc: 0.9876\n",
      "Epoch 39/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0402 - acc: 0.9870 - val_loss: 0.0388 - val_acc: 0.9891\n",
      "Epoch 40/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0408 - acc: 0.9877 - val_loss: 0.0434 - val_acc: 0.9871\n",
      "Epoch 41/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0413 - acc: 0.9874 - val_loss: 0.0333 - val_acc: 0.9901\n",
      "Epoch 42/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0412 - acc: 0.9877 - val_loss: 0.0311 - val_acc: 0.9911\n",
      "Epoch 43/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0417 - acc: 0.9866 - val_loss: 0.0471 - val_acc: 0.9880\n",
      "Epoch 44/200\n",
      "65/65 [==============================] - 9s 135ms/step - loss: 0.0405 - acc: 0.9876 - val_loss: 0.0343 - val_acc: 0.9905\n",
      "Epoch 45/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0364 - acc: 0.9888 - val_loss: 0.0308 - val_acc: 0.9907\n",
      "Epoch 46/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0382 - acc: 0.9888 - val_loss: 0.0372 - val_acc: 0.9882\n",
      "Epoch 47/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0363 - acc: 0.9883 - val_loss: 0.0367 - val_acc: 0.9902\n",
      "Epoch 48/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0372 - acc: 0.9889 - val_loss: 0.0437 - val_acc: 0.9878\n",
      "Epoch 49/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0332 - acc: 0.9898 - val_loss: 0.0381 - val_acc: 0.9883\n",
      "Epoch 50/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0345 - acc: 0.9892 - val_loss: 0.0314 - val_acc: 0.9910\n",
      "Epoch 51/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0340 - acc: 0.9893 - val_loss: 0.0313 - val_acc: 0.9911\n",
      "Epoch 52/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0356 - acc: 0.9888 - val_loss: 0.0317 - val_acc: 0.9912\n",
      "Epoch 53/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0354 - acc: 0.9891 - val_loss: 0.0326 - val_acc: 0.9910\n",
      "Epoch 54/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0314 - acc: 0.9904 - val_loss: 0.0309 - val_acc: 0.9918\n",
      "Epoch 55/200\n",
      "65/65 [==============================] - 9s 138ms/step - loss: 0.0350 - acc: 0.9891 - val_loss: 0.0338 - val_acc: 0.9892\n",
      "Epoch 56/200\n",
      "65/65 [==============================] - 9s 135ms/step - loss: 0.0311 - acc: 0.9904 - val_loss: 0.0298 - val_acc: 0.9909\n",
      "Epoch 57/200\n",
      "65/65 [==============================] - 9s 135ms/step - loss: 0.0337 - acc: 0.9902 - val_loss: 0.0365 - val_acc: 0.9892\n",
      "Epoch 58/200\n",
      "65/65 [==============================] - 9s 137ms/step - loss: 0.0343 - acc: 0.9898 - val_loss: 0.0296 - val_acc: 0.9916\n",
      "Epoch 59/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0304 - acc: 0.9909 - val_loss: 0.0441 - val_acc: 0.9880\n",
      "Epoch 60/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0319 - acc: 0.9904 - val_loss: 0.0307 - val_acc: 0.9907\n",
      "Epoch 61/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0329 - acc: 0.9896 - val_loss: 0.0374 - val_acc: 0.9919\n",
      "Epoch 62/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 8s 130ms/step - loss: 0.0307 - acc: 0.9903 - val_loss: 0.0255 - val_acc: 0.9929\n",
      "Epoch 63/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0314 - acc: 0.9904 - val_loss: 0.0381 - val_acc: 0.9897\n",
      "Epoch 64/200\n",
      "65/65 [==============================] - 8s 130ms/step - loss: 0.0322 - acc: 0.9897 - val_loss: 0.0270 - val_acc: 0.9923\n",
      "Epoch 65/200\n",
      "65/65 [==============================] - 9s 131ms/step - loss: 0.0288 - acc: 0.9908 - val_loss: 0.0303 - val_acc: 0.9909\n",
      "Epoch 66/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0303 - acc: 0.9905 - val_loss: 0.0367 - val_acc: 0.9892\n",
      "Epoch 67/200\n",
      "65/65 [==============================] - 9s 131ms/step - loss: 0.0299 - acc: 0.9907 - val_loss: 0.0301 - val_acc: 0.9921\n",
      "Epoch 68/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0298 - acc: 0.9905 - val_loss: 0.0324 - val_acc: 0.9907\n",
      "Epoch 69/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0296 - acc: 0.9905 - val_loss: 0.0314 - val_acc: 0.9911\n",
      "Epoch 70/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0304 - acc: 0.9897 - val_loss: 0.0345 - val_acc: 0.9901\n",
      "Epoch 71/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0283 - acc: 0.9916 - val_loss: 0.0238 - val_acc: 0.9933\n",
      "Epoch 72/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0283 - acc: 0.9908 - val_loss: 0.0318 - val_acc: 0.9900\n",
      "Epoch 73/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0268 - acc: 0.9916 - val_loss: 0.0327 - val_acc: 0.9920\n",
      "Epoch 74/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0270 - acc: 0.9917 - val_loss: 0.0358 - val_acc: 0.9901\n",
      "Epoch 75/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0291 - acc: 0.9912 - val_loss: 0.0292 - val_acc: 0.9921\n",
      "Epoch 76/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0295 - acc: 0.9906 - val_loss: 0.0456 - val_acc: 0.9868\n",
      "Epoch 77/200\n",
      "65/65 [==============================] - 10s 150ms/step - loss: 0.0264 - acc: 0.9920 - val_loss: 0.0377 - val_acc: 0.9886\n",
      "Epoch 78/200\n",
      "65/65 [==============================] - 9s 146ms/step - loss: 0.0269 - acc: 0.9919 - val_loss: 0.0382 - val_acc: 0.9890\n",
      "Epoch 79/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0255 - acc: 0.9912 - val_loss: 0.0288 - val_acc: 0.9913\n",
      "Epoch 80/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0264 - acc: 0.9914 - val_loss: 0.0368 - val_acc: 0.9907\n",
      "Epoch 81/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0262 - acc: 0.9915 - val_loss: 0.0406 - val_acc: 0.9897\n",
      "Epoch 82/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0253 - acc: 0.9919 - val_loss: 0.0373 - val_acc: 0.9907\n",
      "Epoch 83/200\n",
      "65/65 [==============================] - 9s 135ms/step - loss: 0.0263 - acc: 0.9913 - val_loss: 0.0359 - val_acc: 0.9894\n",
      "Epoch 84/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0254 - acc: 0.9920 - val_loss: 0.0294 - val_acc: 0.9918\n",
      "Epoch 85/200\n",
      "65/65 [==============================] - 9s 131ms/step - loss: 0.0262 - acc: 0.9920 - val_loss: 0.0370 - val_acc: 0.9910\n",
      "Epoch 86/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0247 - acc: 0.9919 - val_loss: 0.0334 - val_acc: 0.9915\n",
      "Epoch 87/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0280 - acc: 0.9908 - val_loss: 0.0332 - val_acc: 0.9910\n",
      "Epoch 88/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0256 - acc: 0.9923 - val_loss: 0.0330 - val_acc: 0.9910\n",
      "Epoch 89/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0239 - acc: 0.9924 - val_loss: 0.0260 - val_acc: 0.9920\n",
      "Epoch 90/200\n",
      "65/65 [==============================] - 9s 139ms/step - loss: 0.0253 - acc: 0.9916 - val_loss: 0.0347 - val_acc: 0.9911\n",
      "Epoch 91/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0281 - acc: 0.9907 - val_loss: 0.0253 - val_acc: 0.9930\n",
      "Epoch 92/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0271 - acc: 0.9912 - val_loss: 0.0410 - val_acc: 0.9888\n",
      "Epoch 93/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0260 - acc: 0.9915 - val_loss: 0.0317 - val_acc: 0.9909\n",
      "Epoch 94/200\n",
      "65/65 [==============================] - 9s 135ms/step - loss: 0.0221 - acc: 0.9929 - val_loss: 0.0422 - val_acc: 0.9891\n",
      "Epoch 95/200\n",
      "65/65 [==============================] - 9s 136ms/step - loss: 0.0235 - acc: 0.9927 - val_loss: 0.0312 - val_acc: 0.9911\n",
      "Epoch 96/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0245 - acc: 0.9923 - val_loss: 0.0297 - val_acc: 0.9928\n",
      "Epoch 97/200\n",
      "65/65 [==============================] - 9s 135ms/step - loss: 0.0244 - acc: 0.9925 - val_loss: 0.0323 - val_acc: 0.9920\n",
      "Epoch 98/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0225 - acc: 0.9926 - val_loss: 0.0346 - val_acc: 0.9900\n",
      "Epoch 99/200\n",
      "65/65 [==============================] - 9s 136ms/step - loss: 0.0242 - acc: 0.9924 - val_loss: 0.0385 - val_acc: 0.9901\n",
      "Epoch 100/200\n",
      "65/65 [==============================] - 9s 135ms/step - loss: 0.0213 - acc: 0.9930 - val_loss: 0.0373 - val_acc: 0.9911\n",
      "Epoch 101/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0250 - acc: 0.9922 - val_loss: 0.0277 - val_acc: 0.9918\n",
      "Epoch 102/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0210 - acc: 0.9935 - val_loss: 0.0364 - val_acc: 0.9907\n",
      "Epoch 103/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0211 - acc: 0.9931 - val_loss: 0.0272 - val_acc: 0.9922\n",
      "Epoch 104/200\n",
      "65/65 [==============================] - 9s 135ms/step - loss: 0.0229 - acc: 0.9925 - val_loss: 0.0295 - val_acc: 0.9914\n",
      "Epoch 105/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0246 - acc: 0.9918 - val_loss: 0.0355 - val_acc: 0.9899\n",
      "Epoch 106/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0218 - acc: 0.9933 - val_loss: 0.0307 - val_acc: 0.9906\n",
      "Epoch 107/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0221 - acc: 0.9929 - val_loss: 0.0286 - val_acc: 0.9915\n",
      "Epoch 108/200\n",
      "65/65 [==============================] - 9s 135ms/step - loss: 0.0235 - acc: 0.9926 - val_loss: 0.0334 - val_acc: 0.9923\n",
      "Epoch 109/200\n",
      "65/65 [==============================] - 9s 135ms/step - loss: 0.0220 - acc: 0.9929 - val_loss: 0.0292 - val_acc: 0.9910\n",
      "Epoch 110/200\n",
      "65/65 [==============================] - 9s 135ms/step - loss: 0.0188 - acc: 0.9942 - val_loss: 0.0327 - val_acc: 0.9911\n",
      "Epoch 111/200\n",
      "65/65 [==============================] - 9s 135ms/step - loss: 0.0233 - acc: 0.9926 - val_loss: 0.0284 - val_acc: 0.9910\n",
      "Epoch 112/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0210 - acc: 0.9932 - val_loss: 0.0329 - val_acc: 0.9905\n",
      "Epoch 113/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0203 - acc: 0.9932 - val_loss: 0.0372 - val_acc: 0.9914\n",
      "Epoch 114/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0201 - acc: 0.9930 - val_loss: 0.0361 - val_acc: 0.9896\n",
      "Epoch 115/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0217 - acc: 0.9929 - val_loss: 0.0354 - val_acc: 0.9913\n",
      "Epoch 116/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0191 - acc: 0.9939 - val_loss: 0.0262 - val_acc: 0.9924\n",
      "Epoch 117/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0225 - acc: 0.9921 - val_loss: 0.0306 - val_acc: 0.9924\n",
      "Epoch 118/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0196 - acc: 0.9936 - val_loss: 0.0266 - val_acc: 0.9928\n",
      "Epoch 119/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0227 - acc: 0.9928 - val_loss: 0.0372 - val_acc: 0.9906\n",
      "Epoch 120/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0209 - acc: 0.9938 - val_loss: 0.0509 - val_acc: 0.9855\n",
      "Epoch 121/200\n",
      "65/65 [==============================] - 9s 135ms/step - loss: 0.0208 - acc: 0.9938 - val_loss: 0.0381 - val_acc: 0.9895\n",
      "Epoch 122/200\n",
      "65/65 [==============================] - 9s 136ms/step - loss: 0.0197 - acc: 0.9931 - val_loss: 0.0305 - val_acc: 0.9915\n",
      "Epoch 123/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0215 - acc: 0.9929 - val_loss: 0.0356 - val_acc: 0.9906\n",
      "Epoch 124/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0226 - acc: 0.9926 - val_loss: 0.0329 - val_acc: 0.9916\n",
      "Epoch 125/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0203 - acc: 0.9931 - val_loss: 0.0302 - val_acc: 0.9932\n",
      "Epoch 126/200\n",
      "65/65 [==============================] - 8s 131ms/step - loss: 0.0221 - acc: 0.9931 - val_loss: 0.0295 - val_acc: 0.9910\n",
      "Epoch 127/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0194 - acc: 0.9937 - val_loss: 0.0235 - val_acc: 0.9938\n",
      "Epoch 128/200\n",
      "65/65 [==============================] - 9s 131ms/step - loss: 0.0215 - acc: 0.9930 - val_loss: 0.0295 - val_acc: 0.9925\n",
      "Epoch 129/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0184 - acc: 0.9936 - val_loss: 0.0360 - val_acc: 0.9916\n",
      "Epoch 130/200\n",
      "65/65 [==============================] - 9s 131ms/step - loss: 0.0191 - acc: 0.9939 - val_loss: 0.0317 - val_acc: 0.9910\n",
      "Epoch 131/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0222 - acc: 0.9935 - val_loss: 0.0326 - val_acc: 0.9920\n",
      "Epoch 132/200\n",
      "65/65 [==============================] - 8s 130ms/step - loss: 0.0185 - acc: 0.9940 - val_loss: 0.0261 - val_acc: 0.9932\n",
      "Epoch 133/200\n",
      "65/65 [==============================] - 9s 131ms/step - loss: 0.0193 - acc: 0.9938 - val_loss: 0.0386 - val_acc: 0.9904\n",
      "Epoch 134/200\n",
      "65/65 [==============================] - 9s 131ms/step - loss: 0.0202 - acc: 0.9932 - val_loss: 0.0310 - val_acc: 0.9934\n",
      "Epoch 135/200\n",
      "65/65 [==============================] - 8s 131ms/step - loss: 0.0219 - acc: 0.9928 - val_loss: 0.0354 - val_acc: 0.9905\n",
      "Epoch 136/200\n",
      "65/65 [==============================] - 8s 129ms/step - loss: 0.0193 - acc: 0.9936 - val_loss: 0.0316 - val_acc: 0.9910\n",
      "Epoch 137/200\n",
      "65/65 [==============================] - 8s 130ms/step - loss: 0.0195 - acc: 0.9939 - val_loss: 0.0328 - val_acc: 0.9917\n",
      "Epoch 138/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0194 - acc: 0.9936 - val_loss: 0.0311 - val_acc: 0.9921\n",
      "Epoch 139/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0189 - acc: 0.9941 - val_loss: 0.0278 - val_acc: 0.9929\n",
      "Epoch 140/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0188 - acc: 0.9941 - val_loss: 0.0314 - val_acc: 0.9923\n",
      "Epoch 141/200\n",
      "65/65 [==============================] - 9s 131ms/step - loss: 0.0193 - acc: 0.9937 - val_loss: 0.0375 - val_acc: 0.9906\n",
      "Epoch 142/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0196 - acc: 0.9935 - val_loss: 0.0407 - val_acc: 0.9885\n",
      "Epoch 143/200\n",
      "65/65 [==============================] - 9s 131ms/step - loss: 0.0187 - acc: 0.9942 - val_loss: 0.0335 - val_acc: 0.9909\n",
      "Epoch 144/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0188 - acc: 0.9942 - val_loss: 0.0243 - val_acc: 0.9933\n",
      "Epoch 145/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0172 - acc: 0.9944 - val_loss: 0.0340 - val_acc: 0.9913\n",
      "Epoch 146/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0192 - acc: 0.9939 - val_loss: 0.0255 - val_acc: 0.9926\n",
      "Epoch 147/200\n",
      "65/65 [==============================] - 8s 131ms/step - loss: 0.0194 - acc: 0.9937 - val_loss: 0.0340 - val_acc: 0.9915\n",
      "Epoch 148/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0177 - acc: 0.9945 - val_loss: 0.0305 - val_acc: 0.9916\n",
      "Epoch 149/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0174 - acc: 0.9941 - val_loss: 0.0305 - val_acc: 0.9923\n",
      "Epoch 150/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0201 - acc: 0.9936 - val_loss: 0.0335 - val_acc: 0.9904\n",
      "Epoch 151/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0186 - acc: 0.9941 - val_loss: 0.0344 - val_acc: 0.9919\n",
      "Epoch 152/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0200 - acc: 0.9935 - val_loss: 0.0271 - val_acc: 0.9929\n",
      "Epoch 153/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0193 - acc: 0.9936 - val_loss: 0.0277 - val_acc: 0.9935\n",
      "Epoch 154/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0159 - acc: 0.9948 - val_loss: 0.0296 - val_acc: 0.9918\n",
      "Epoch 155/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0192 - acc: 0.9939 - val_loss: 0.0282 - val_acc: 0.9935\n",
      "Epoch 156/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0179 - acc: 0.9940 - val_loss: 0.0210 - val_acc: 0.9935\n",
      "Epoch 157/200\n",
      "65/65 [==============================] - 9s 131ms/step - loss: 0.0167 - acc: 0.9942 - val_loss: 0.0305 - val_acc: 0.9925\n",
      "Epoch 158/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0175 - acc: 0.9946 - val_loss: 0.0320 - val_acc: 0.9914\n",
      "Epoch 159/200\n",
      "65/65 [==============================] - 9s 131ms/step - loss: 0.0165 - acc: 0.9948 - val_loss: 0.0280 - val_acc: 0.9916\n",
      "Epoch 160/200\n",
      "65/65 [==============================] - 9s 136ms/step - loss: 0.0170 - acc: 0.9947 - val_loss: 0.0347 - val_acc: 0.9904\n",
      "Epoch 161/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0174 - acc: 0.9948 - val_loss: 0.0327 - val_acc: 0.9916\n",
      "Epoch 162/200\n",
      "65/65 [==============================] - 8s 130ms/step - loss: 0.0171 - acc: 0.9942 - val_loss: 0.0336 - val_acc: 0.9924\n",
      "Epoch 163/200\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.0164 - acc: 0.9949 - val_loss: 0.0272 - val_acc: 0.9920\n",
      "Epoch 164/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0175 - acc: 0.9934 - val_loss: 0.0271 - val_acc: 0.9929\n",
      "Epoch 165/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0163 - acc: 0.9948 - val_loss: 0.0330 - val_acc: 0.9901\n",
      "Epoch 166/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0162 - acc: 0.9945 - val_loss: 0.0248 - val_acc: 0.9925\n",
      "Epoch 167/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0179 - acc: 0.9945 - val_loss: 0.0305 - val_acc: 0.9920\n",
      "Epoch 168/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0178 - acc: 0.9941 - val_loss: 0.0239 - val_acc: 0.9940\n",
      "Epoch 169/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0175 - acc: 0.9945 - val_loss: 0.0295 - val_acc: 0.9914\n",
      "Epoch 170/200\n",
      "65/65 [==============================] - 9s 136ms/step - loss: 0.0168 - acc: 0.9945 - val_loss: 0.0250 - val_acc: 0.9935\n",
      "Epoch 171/200\n",
      "65/65 [==============================] - 9s 137ms/step - loss: 0.0166 - acc: 0.9943 - val_loss: 0.0267 - val_acc: 0.9934\n",
      "Epoch 172/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0164 - acc: 0.9947 - val_loss: 0.0287 - val_acc: 0.9924\n",
      "Epoch 173/200\n",
      "65/65 [==============================] - 10s 154ms/step - loss: 0.0164 - acc: 0.9946 - val_loss: 0.0272 - val_acc: 0.9933\n",
      "Epoch 174/200\n",
      "65/65 [==============================] - 9s 135ms/step - loss: 0.0182 - acc: 0.9941 - val_loss: 0.0326 - val_acc: 0.9925\n",
      "Epoch 175/200\n",
      "65/65 [==============================] - 9s 135ms/step - loss: 0.0166 - acc: 0.9949 - val_loss: 0.0275 - val_acc: 0.9935\n",
      "Epoch 176/200\n",
      "65/65 [==============================] - 9s 135ms/step - loss: 0.0200 - acc: 0.9932 - val_loss: 0.0260 - val_acc: 0.9934\n",
      "Epoch 177/200\n",
      "65/65 [==============================] - 9s 137ms/step - loss: 0.0168 - acc: 0.9947 - val_loss: 0.0183 - val_acc: 0.9952\n",
      "Epoch 178/200\n",
      "65/65 [==============================] - 9s 137ms/step - loss: 0.0158 - acc: 0.9951 - val_loss: 0.0320 - val_acc: 0.9921\n",
      "Epoch 179/200\n",
      "65/65 [==============================] - 9s 136ms/step - loss: 0.0166 - acc: 0.9945 - val_loss: 0.0313 - val_acc: 0.9924\n",
      "Epoch 180/200\n",
      "65/65 [==============================] - 9s 136ms/step - loss: 0.0155 - acc: 0.9949 - val_loss: 0.0298 - val_acc: 0.9920\n",
      "Epoch 181/200\n",
      "65/65 [==============================] - 9s 135ms/step - loss: 0.0156 - acc: 0.9948 - val_loss: 0.0212 - val_acc: 0.9949\n",
      "Epoch 182/200\n",
      "65/65 [==============================] - 9s 135ms/step - loss: 0.0166 - acc: 0.9947 - val_loss: 0.0306 - val_acc: 0.9910\n",
      "Epoch 183/200\n",
      "65/65 [==============================] - 9s 136ms/step - loss: 0.0159 - acc: 0.9945 - val_loss: 0.0295 - val_acc: 0.9930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0147 - acc: 0.9952 - val_loss: 0.0247 - val_acc: 0.9923\n",
      "Epoch 185/200\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.0158 - acc: 0.9949 - val_loss: 0.0263 - val_acc: 0.9929\n",
      "Epoch 186/200\n",
      "65/65 [==============================] - 9s 135ms/step - loss: 0.0189 - acc: 0.9941 - val_loss: 0.0291 - val_acc: 0.9913\n",
      "Epoch 187/200\n",
      "65/65 [==============================] - 11s 172ms/step - loss: 0.0147 - acc: 0.9950 - val_loss: 0.0262 - val_acc: 0.9938\n",
      "Epoch 188/200\n",
      "65/65 [==============================] - 8s 129ms/step - loss: 0.0159 - acc: 0.9949 - val_loss: 0.0270 - val_acc: 0.9932\n",
      "Epoch 189/200\n",
      "65/65 [==============================] - 8s 131ms/step - loss: 0.0172 - acc: 0.9947 - val_loss: 0.0281 - val_acc: 0.9916\n",
      "Epoch 190/200\n",
      "65/65 [==============================] - 8s 128ms/step - loss: 0.0163 - acc: 0.9942 - val_loss: 0.0353 - val_acc: 0.9915\n",
      "Epoch 191/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0158 - acc: 0.9952 - val_loss: 0.0298 - val_acc: 0.9921\n",
      "Epoch 192/200\n",
      "65/65 [==============================] - 9s 131ms/step - loss: 0.0158 - acc: 0.9950 - val_loss: 0.0266 - val_acc: 0.9937\n",
      "Epoch 193/200\n",
      "65/65 [==============================] - 8s 128ms/step - loss: 0.0140 - acc: 0.9954 - val_loss: 0.0320 - val_acc: 0.9929\n",
      "Epoch 194/200\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.0141 - acc: 0.9954 - val_loss: 0.0283 - val_acc: 0.9918\n",
      "Epoch 195/200\n",
      "65/65 [==============================] - 9s 131ms/step - loss: 0.0153 - acc: 0.9952 - val_loss: 0.0284 - val_acc: 0.9924\n",
      "Epoch 196/200\n",
      "65/65 [==============================] - 11s 164ms/step - loss: 0.0158 - acc: 0.9951 - val_loss: 0.0305 - val_acc: 0.9925\n",
      "Epoch 197/200\n",
      "65/65 [==============================] - 15s 237ms/step - loss: 0.0146 - acc: 0.9949 - val_loss: 0.0249 - val_acc: 0.9938\n",
      "Epoch 198/200\n",
      "65/65 [==============================] - 15s 237ms/step - loss: 0.0148 - acc: 0.9953 - val_loss: 0.0286 - val_acc: 0.9926\n",
      "Epoch 199/200\n",
      "65/65 [==============================] - 15s 237ms/step - loss: 0.0151 - acc: 0.9949 - val_loss: 0.0313 - val_acc: 0.9904\n",
      "Epoch 200/200\n",
      "65/65 [==============================] - 15s 236ms/step - loss: 0.0154 - acc: 0.9950 - val_loss: 0.0289 - val_acc: 0.9924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26da1c25b70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow(X_train,Y_train,batch_size=512),epochs = 200,steps_per_epoch = 33600//512,validation_data = valgen.flow(X_test,Y_test,batch_size = 512),validation_steps = 8400//512,callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 6)         156       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 24, 24, 6)         24        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 16)        2416      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 12, 12, 16)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               69240     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 120)               480       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 84)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 84)                336       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 83,730\n",
      "Trainable params: 83,278\n",
      "Non-trainable params: 452\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"mnist.best.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33600/33600 [==============================] - 12s 361us/step\n",
      "8400/8400 [==============================] - 3s 378us/step\n"
     ]
    }
   ],
   "source": [
    "at = model.evaluate(X_train/255,Y_train)[1]\n",
    "att = model.evaluate(X_test/255,Y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy 99.84226190476191\n",
      "Test set accuracy 99.3452380952381\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set accuracy\",at*100)\n",
    "print(\"Test set accuracy\",att*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
